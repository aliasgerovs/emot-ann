import gradio as gr
import os
import pandas as pd
from datetime import datetime
import shutil
import time
import gc
from contextlib import contextmanager
from moviepy.editor import VideoFileClip
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed

class VideoEmotionAnnotator:
    def __init__(self):
        # Use absolute paths in current working directory
        self.base_dir = os.path.abspath(os.getcwd())
        self.clips_dir = None  # Set dynamically per session
        self.working_dir = os.path.join(self.base_dir, 'annotation_working')
        self.uploads_dir = os.path.join(self.base_dir, 'annotation_uploads')
        self.annotation_clips_dir = os.path.join(self.base_dir, 'annotation_clips')
        
        # Create directories (clips_dir will be temp)
        os.makedirs(self.working_dir, exist_ok=True)
        os.makedirs(self.uploads_dir, exist_ok=True)
        os.makedirs(self.annotation_clips_dir, exist_ok=True)
        
        print(f"Using working directory: {self.working_dir}")
        print(f"Using uploads directory: {self.uploads_dir}")
        print(f"Using annotation clips directory: {self.annotation_clips_dir}")
        
        # Check FFmpeg availability
        self.ffmpeg_available = shutil.which('ffprobe') is not None
        
        if self.ffmpeg_available:
            print("FFmpeg detected - using for video info")
        else:
            print("FFmpeg not detected - falling back to MoviePy for video info")
        
        self.annotations = []
        self.current_clips = []
        self.participant_id = ""
        self.working_video_path = None
        
    def __del__(self):
        try:
            if hasattr(self, 'working_dir') and os.path.exists(self.working_dir):
                for file in os.listdir(self.working_dir):
                    try:
                        file_path = os.path.join(self.working_dir, file)
                        if os.path.isfile(file_path):
                            os.remove(file_path)
                    except:
                        pass
            
            # Clean clips from temp session dir if still there
            if hasattr(self, 'clips_dir') and self.clips_dir and os.path.exists(self.clips_dir):
                for file in os.listdir(self.clips_dir):
                    if file.startswith('clip_'):  # Only our clips
                        file_path = os.path.join(self.clips_dir, file)
                        if os.path.isfile(file_path):
                            os.remove(file_path)
                            time.sleep(0.1)
        except Exception as e:
            print(f"Error cleaning up files: {str(e)}")
    
    @contextmanager
    def video_clip_context(self, path):
        """Context manager for MoviePy VideoFileClip"""
        clip = None
        try:
            clip = VideoFileClip(path)
            yield clip
        finally:
            if clip is not None:
                clip.close()
            time.sleep(0.2)
    
    def _get_video_info_ffmpeg(self, path):
        """Get video info using ffprobe - only if available"""
        if not self.ffmpeg_available:
            return 0, 0, 0
        
        try:
            # Get duration
            cmd_duration = [
                'ffprobe',
                '-v', 'error',
                '-show_entries', 'format=duration',
                '-of', 'csv=p=0',
                path
            ]
            duration_result = subprocess.run(cmd_duration, capture_output=True, text=True, check=True)
            duration = float(duration_result.stdout.strip())
            
            # Get FPS from video stream
            cmd_fps = [
                'ffprobe',
                '-v', 'error',
                '-select_streams', 'v:0',
                '-show_entries', 'stream=r_frame_rate',
                '-of', 'csv=p=0',
                path
            ]
            fps_result = subprocess.run(cmd_fps, capture_output=True, text=True, check=True)
            r_frame_rate = fps_result.stdout.strip()
            if '/' in r_frame_rate:
                num, den = map(int, r_frame_rate.split('/'))
                fps = num / den if den != 0 else 30.0  # default fallback
            else:
                fps = float(r_frame_rate) if r_frame_rate else 30.0
            
            frame_count = int(duration * fps)
            return duration, fps, frame_count
        except Exception as e:
            print(f"ffprobe error: {e}")
            return 0, 0, 0
    
    def get_video_info(self, video_path):
        if not video_path:
            return 0, 0, 0
        
        working_path = self.copy_video_to_working_dir(video_path)
        if not working_path:
            return 0, 0, 0
        
        # Try ffprobe first if available
        duration, fps, frame_count = self._get_video_info_ffmpeg(working_path)
        if duration > 0:
            return duration, fps, frame_count
        
        # Fallback to MoviePy
        try:
            time.sleep(0.3)
            with self.video_clip_context(working_path) as clip:
                duration = clip.duration
                fps = clip.fps
                frame_count = int(duration * fps)
                return duration, fps, frame_count
        except Exception as e:
            print(f"Error getting video info: {e}")
            import traceback
            traceback.print_exc()
        
        return 0, 0, 0

    def format_time(self, seconds):
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes:02d}:{secs:02d}"
    
    def copy_video_to_working_dir(self, video_path):
        """Copy video from Gradio temp to our working directory"""
        if not video_path or not os.path.exists(video_path):
            print(f"Video path does not exist: {video_path}")
            return None
        
        try:
            # Get just the filename
            filename = os.path.basename(video_path)
            # Add timestamp to avoid conflicts
            timestamp = int(time.time() * 1000)
            # Save to our uploads directory first
            upload_path = os.path.join(self.uploads_dir, f"upload_{timestamp}_{filename}")
            
            print(f"Copying from: {video_path}")
            print(f"Copying to: {upload_path}")
            
            # Copy file in chunks
            with open(video_path, 'rb') as src:
                with open(upload_path, 'wb') as dst:
                    shutil.copyfileobj(src, dst, length=1024*1024)
            
            if not os.path.exists(upload_path):
                print("Error: Upload file was not created")
                return None
            
            file_size = os.path.getsize(upload_path)
            print(f"Uploaded file size: {file_size} bytes")
            
            # Now copy to working directory
            working_filename = f"working_{timestamp}_{filename}"
            working_path = os.path.join(self.working_dir, working_filename)
            
            print(f"Copying to working: {working_path}")
            shutil.copy2(upload_path, working_path)
            
            if os.path.exists(working_path):
                print(f"Successfully copied to working directory: {working_path}")
                time.sleep(0.3)
                return working_path
            else:
                print("Error: Working file was not created")
                return None
                
        except Exception as e:
            print(f"Error copying video: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def process_video(self, video_file, participant_id, start_time, interval, process_minutes, progress=gr.Progress()):
        if not video_file or not participant_id:
            return "Please upload a video and provide a Participant ID.", gr.update(choices=[])
        
        progress(0, desc="Starting video processing...")
        
        # Get the video path from Gradio
        if hasattr(video_file, 'name'):
            video_path = video_file.name
        elif isinstance(video_file, str):
            video_path = video_file
        else:
            return "Invalid video file.", gr.update(choices=[])
        
        print(f"Received video path: {video_path}")
        
        if not os.path.exists(video_path):
            return f"Video file not found: {video_path}", gr.update(choices=[])
            
        self.participant_id = participant_id
        
        progress(0.05, desc="Cleaning up old files...")
        
        # Clean up old clips
        for clip in self.current_clips:
            try:
                if os.path.exists(clip['path']):
                    os.remove(clip['path'])
                    time.sleep(0.1)
            except Exception as e:
                print(f"Could not remove old clip: {str(e)}")
        
        # Clean up old working video
        if self.working_video_path and os.path.exists(self.working_video_path):
            try:
                os.remove(self.working_video_path)
                time.sleep(0.3)
            except Exception as e:
                print(f"Could not remove old working video: {str(e)}")
        
        progress(0.1, desc="Copying video to working directory...")
        
        # Copy video to our controlled directory
        self.working_video_path = self.copy_video_to_working_dir(video_path)
        if not self.working_video_path:
            return "Failed to prepare video file. Please try again.", gr.update(choices=[])
        
        # Create clips directory with video name
        video_filename = os.path.splitext(os.path.basename(self.working_video_path))[0]
        self.clips_dir = os.path.join(self.annotation_clips_dir, video_filename)
        os.makedirs(self.clips_dir, exist_ok=True)
        print(f"Saving clips to: {self.clips_dir}")
        
        time.sleep(0.5)
        
        progress(0.15, desc="Reading video information...")
        
        fps = 0
        duration = 0
        
        # Get info (ffprobe if available, else MoviePy)
        duration, fps, _ = self._get_video_info_ffmpeg(self.working_video_path)
        if duration == 0:
            try:
                with self.video_clip_context(self.working_video_path) as clip:
                    duration = clip.duration
                    fps = clip.fps
            except Exception as e:
                print(f"Error reading video: {e}")
                import traceback
                traceback.print_exc()
                return "Failed to read video file. Please try again.", gr.update(choices=[])
        
        print(f"Video info - Duration: {duration}s, FPS: {fps}")
        
        time.sleep(0.3)
        
        if duration == 0:
            return "Failed to read video file. Please try again.", gr.update(choices=[])
        
        progress(0.25, desc="Calculating clip parameters...")
        
        requested_seconds = (process_minutes or 0) * 60
        if requested_seconds <= 0:
            requested_seconds = 240
        end_time = min(start_time + requested_seconds, duration)
        
        if end_time <= start_time:
            return "Start time is too close to the end of the video. Please choose an earlier start time.", gr.update(choices=[])
        
        time_range_msg = f"Processing from {self.format_time(start_time)} to {self.format_time(end_time)} (duration: {self.format_time(end_time - start_time)})"
        self.annotations = []
        
        progress(0.3, desc="Starting clip extraction...")
        
        try:
            time.sleep(0.3)
            self.current_clips = []
            clip_params_list = []
            current_time = start_time
            clip_num = 1
            while current_time < end_time:
                clip_end_time = min(current_time + interval, end_time)
                clip_params_list.append((current_time, clip_end_time, clip_num))
                current_time += interval
                clip_num += 1
            
            total_clips = len(clip_params_list)
            print(f"Will create {total_clips} clips")

            def create_single_clip(clip_params):
                current_time, clip_end_time, clip_num = clip_params
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
                clip_filename = f"clip_{self.participant_id}_{timestamp}_{clip_num:03d}.mp4"
                output_path = os.path.join(self.clips_dir, clip_filename)
                
                try:
                    with self.video_clip_context(self.working_video_path) as video:
                        subclip = video.subclip(current_time, clip_end_time)
                        
                        subclip.write_videofile(
                            output_path,
                            codec='libx264',
                            audio_codec='aac',
                            ffmpeg_params=['-movflags', '+faststart'],
                            logger=None,
                            threads=1,
                            preset='ultrafast'
                        )
                        subclip.close()
                    
                    time.sleep(2.0)
                    
                    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                        clip_label = f"Clip {clip_num:03d} ({self.format_time(current_time)} - {self.format_time(clip_end_time)})"
                        return {
                            'label': clip_label,
                            'path': output_path,
                            'start': current_time,
                            'end': clip_end_time,
                            'number': clip_num
                        }
                    else:
                        return None
                
                except Exception as e:
                    error_str = str(e)
                    if 'stdout' in error_str and 'NoneType' in error_str:
                        print(f"Ignoring known audio processing error for clip {clip_num}: {error_str}")
                        time.sleep(2.0)
                        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                            clip_label = f"Clip {clip_num:03d} ({self.format_time(current_time)} - {self.format_time(clip_end_time)})"
                            return {
                                'label': clip_label,
                                'path': output_path,
                                'start': current_time,
                                'end': clip_end_time,
                                'number': clip_num
                            }
                        else:
                            return None
                    else:
                        print(f"Unexpected error creating clip {clip_num}: {str(e)}")
                        import traceback
                        traceback.print_exc()
                        return None
            
            progress(0.3, desc="Starting parallel clip extraction...")
            
            with ThreadPoolExecutor(max_workers=30) as executor:
                future_to_clipnum = {executor.submit(create_single_clip, params): params[2] for params in clip_params_list}
                completed = 0
                for future in as_completed(future_to_clipnum):
                    try:
                        result = future.result()
                        if result:
                            self.current_clips.append(result)
                            print(f"Successfully created clip {result['number']}")
                        completed += 1
                        progress(
                            0.3 + (0.65 * completed / total_clips),
                            desc=f"Extracted {completed}/{total_clips} clips"
                        )
                    except Exception as exc:
                        print(f"Error in clip future: {exc}")
                        completed += 1
                        progress(
                            0.3 + (0.65 * completed / total_clips),
                            desc=f"Extracted {completed}/{total_clips} clips (some errors)"
                        )
            
            self.current_clips.sort(key=lambda x: x['number'])
        
        except Exception as e:
            print(f"Error during clip extraction: {str(e)}")
            import traceback
            traceback.print_exc()
            return f"Failed to process video: {str(e)}", gr.update(choices=[])
        
        progress(0.95, desc="Finalizing...")
        time.sleep(0.5)
        
        if not self.current_clips:
            return "No clips were successfully created. Please try again.", gr.update(choices=[])
        
        clip_choices = [clip['label'] for clip in self.current_clips]
        
        progress(1.0, desc="Complete!")
        
        success_msg = f"""‚úÖ **Video processed successfully!**

{time_range_msg}

üìä **Results:**
- Created {len(self.current_clips)} clips
- Clip interval: {interval} seconds
- Ready for annotation

Select a clip below to begin annotating."""
        
        return success_msg, gr.update(choices=clip_choices, value=clip_choices[0] if clip_choices else None)

    def load_clip(self, clip_label):
        if not clip_label:
            return None
        
        for clip in self.current_clips:
            if clip['label'] == clip_label:
                if os.path.exists(clip['path']):
                    full_path = clip['path']  # Just return the full temp path
                    print(f"Loading clip full path: {full_path}")
                    return full_path  # Gradio Video will serve it like the original
        
        return None
    
    def save_annotation(self, clip_label, has_no_emotion, sadness, anger, pleasure, task_type, annotator_name):
        if not clip_label:
            return "‚ö†Ô∏è Please select a clip first."
        
        if not task_type or not annotator_name:
            return "‚ö†Ô∏è Please fill in Task Type and Annotator Name."
        
        clip_info = None
        for clip in self.current_clips:
            if clip['label'] == clip_label:
                clip_info = clip
                break
        
        if not clip_info:
            return "‚ö†Ô∏è Clip not found."
        
        annotation = {
            'ParticipantID': self.participant_id,
            'TaskType': task_type,
            'AnnotatorName': annotator_name,
            'ClipNumber': clip_info['number'],
            'StartTime': self.format_time(clip_info['start']),
            'EndTime': self.format_time(clip_info['end']),
            'NC': 1 if has_no_emotion else 0,
            'Sadness': 0 if has_no_emotion else sadness,
            'Anger': 0 if has_no_emotion else anger,
            'Pleasure': 0 if has_no_emotion else pleasure,
            'Timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        existing_idx = None
        for idx, ann in enumerate(self.annotations):
            if (ann['ParticipantID'] == annotation['ParticipantID'] and 
                ann['ClipNumber'] == annotation['ClipNumber'] and
                ann['TaskType'] == annotation['TaskType'] and
                ann['AnnotatorName'] == annotation['AnnotatorName']):
                existing_idx = idx
                break
        
        if existing_idx is not None:
            self.annotations[existing_idx] = annotation
            return f"‚úÖ **Annotation updated!** (Clip {clip_info['number']}/{len(self.current_clips)})"
        else:
            self.annotations.append(annotation)
            return f"‚úÖ **Annotation saved!** ({len(self.annotations)}/{len(self.current_clips)} clips annotated)"
    
    def export_csv(self):
        if not self.annotations:
            return None, "‚ö†Ô∏è No annotations to export. Please annotate some clips first."
        
        try:
            df = pd.DataFrame(self.annotations)
            df = df.sort_values(['ParticipantID', 'TaskType', 'ClipNumber'])
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"annotations_{self.participant_id}_{timestamp}.csv"
            output_path = os.path.join(self.base_dir, filename)
            
            df.to_csv(output_path, index=False)
            
            return output_path, f"‚úÖ **Exported successfully!** ({len(self.annotations)} annotations saved to {filename})"
        
        except Exception as e:
            return None, f"‚ùå Export failed: {str(e)}"

annotator = VideoEmotionAnnotator()

def on_video_upload(video):
    if not video:
        return "Please upload a video.", 0, 0, 1
    
    video_path = video.name if hasattr(video, 'name') else video
    print(f"Video uploaded: {video_path}")
    
    duration, fps, frame_count = annotator.get_video_info(video_path)
    
    if duration == 0:
        return "‚ö†Ô∏è Could not read video file. Please try a different format.", 0, 0, 1
    
    total_minutes = int(duration // 60)
    remaining_seconds = int(duration % 60)
    
    return (
        f"‚úÖ **Video uploaded successfully!**\n\nüìπ Duration: {total_minutes}:{remaining_seconds:02d} ({duration:.1f}s)\nüéûÔ∏è Frame rate: {fps:.2f} fps",
        0,
        0,
        min(4, max(1, total_minutes))
    )

css = """
#process_btn, #save_btn, #export_btn {
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    border: none;
    color: white;
    font-weight: 600;
}
.step-header {
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 15px;
    border-radius: 10px;
    margin: 20px 0 10px 0;
    font-size: 18px;
    font-weight: 600;
    text-align: center;
}
"""

with gr.Blocks(css=css, title="Video Emotion Annotator") as demo:
    gr.HTML("<div style='text-align: center; padding: 20px;'><h1>üé¨ Video Emotion Annotation Tool</h1><p>Process videos, annotate emotions, and export results</p></div>")
    
    gr.HTML("<div style='max-width: 1400px; margin: 0 auto;'>")
    
    gr.HTML("<div class='step-header'>‚öôÔ∏è Step 1: Upload & Configure</div>")
    
    with gr.Row():
        with gr.Column(scale=3):
            
            with gr.Row():
                video_input = gr.Video(
                    label="Upload Video", 
                    sources=["upload"],
                    include_audio=True
                )
                participant_id = gr.Textbox(label="Participant ID", placeholder="Enter participant identifier")
            
            with gr.Row():
                task_type = gr.Dropdown(label="Task Type", choices=["Traditional", "Digital"], value="Traditional")
                annotator_name = gr.Textbox(label="Annotator Name", placeholder="Enter annotator name")
            
            upload_status = gr.Markdown("", visible=True)
            
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("**Start Time**")
                    with gr.Row():
                        start_minutes = gr.Number(label="Minutes", value=0, minimum=0, precision=0)
                        start_seconds = gr.Number(label="Seconds", value=0, minimum=0, maximum=59, precision=0)
                with gr.Column(scale=1):
                    interval = gr.Number(label="Clip Interval (seconds)", value=5, minimum=1)
                    process_minutes = gr.Number(label="Process Duration (minutes)", value=1, minimum=1)
            
            start_time = gr.Number(value=0, visible=False)
            
            process_btn = gr.Button("üé¨ Process Video", elem_id="process_btn", size="lg")
            process_status = gr.Markdown("")
        
        with gr.Column(scale=2):
            gr.HTML("<div class='step-header'>üìä Progress Overview</div>")
            gr.Markdown("Upload your video and set parameters to begin annotation.")
    
    gr.HTML("<div class='step-header'>üéØ Step 2: Annotate Clips</div>")
    
    with gr.Row():
        with gr.Column(scale=2):
            clip_selector = gr.Dropdown(label="Select Clip", choices=[], interactive=True)
            clip_video = gr.Video(label="Current Clip", height=300, autoplay=False, show_download_button=True)
        
        with gr.Column(scale=1):
            has_emotion = gr.Checkbox(label="NC (No Clear Emotion)", value=False)
            
            gr.Markdown("**Emotion Intensities (1-3):**")
            sadness_intensity = gr.Slider(minimum=0, maximum=3, step=1, label="üò¢ Sadness", value=0, visible=True)
            anger_intensity = gr.Slider(minimum=0, maximum=3, step=1, label="üò† Anger", value=0, visible=True)
            pleasure_intensity = gr.Slider(minimum=0, maximum=3, step=1, label="üòä Pleasure", value=0, visible=True)
            
            save_btn = gr.Button("üíæ Save Annotation", elem_id="save_btn")
            annotation_status = gr.Markdown("")
        
    gr.HTML("<div class='step-header'>üì§ Step 3: Export Results</div>")
    
    with gr.Row():
        export_btn = gr.Button("üìä Export CSV", elem_id="export_btn", size="lg")
        export_file = gr.File(label="Download Results", visible=False)
        export_status = gr.Markdown("")
    
    gr.HTML("</div>")
    
    def update_emotion_fields(nc_checked):
        return (
            gr.update(visible=not nc_checked),
            gr.update(visible=not nc_checked),
            gr.update(visible=not nc_checked)
        )
    
    def calculate_start_time(minutes, seconds):
        return (minutes or 0) * 60 + (seconds or 0)
    
    has_emotion.change(update_emotion_fields, inputs=[has_emotion], outputs=[sadness_intensity, anger_intensity, pleasure_intensity])
    
    video_input.upload(on_video_upload, inputs=[video_input], outputs=[upload_status, start_minutes, start_seconds, process_minutes])
    
    start_minutes.change(calculate_start_time, inputs=[start_minutes, start_seconds], outputs=[start_time])
    start_seconds.change(calculate_start_time, inputs=[start_minutes, start_seconds], outputs=[start_time])
    
    process_btn.click(
        annotator.process_video,
        inputs=[video_input, participant_id, start_time, interval, process_minutes],
        outputs=[process_status, clip_selector],
        show_progress="full",
        api_name="process_video"
    )
    
    def load_clip_and_reset_filters(selected_clip):
        if not selected_clip:
            return (None, False, 0, 0, 0, gr.update(visible=True), gr.update(visible=True), gr.update(visible=True))
        
        video_path = annotator.load_clip(selected_clip)  # Now full temp path
        
        if video_path and os.path.exists(video_path):
            print(f"Clip ready in temp dir: {video_path}")
            time.sleep(0.5)  # Short sleep‚Äîtemps are fast
            return (
                video_path,  # Full path
                False,
                0,
                0,
                0,
                gr.update(visible=True),
                gr.update(visible=True),
                gr.update(visible=True)
            )
        else:
            print(f"Clip load failed: {video_path} (exists: {os.path.exists(video_path) if video_path else 'None'})")
            return (None, False, 0, 0, 0, gr.update(visible=True), gr.update(visible=True), gr.update(visible=True))
    
    clip_selector.change(
        load_clip_and_reset_filters,
        inputs=[clip_selector],
        outputs=[clip_video, has_emotion, sadness_intensity, anger_intensity, pleasure_intensity, 
                sadness_intensity, anger_intensity, pleasure_intensity],
        show_progress="hidden"
    )
    
    save_btn.click(
        annotator.save_annotation,
        inputs=[clip_selector, has_emotion, sadness_intensity, anger_intensity, pleasure_intensity, task_type, annotator_name],
        outputs=[annotation_status]
    )
    
    def handle_export():
        file, status = annotator.export_csv()
        if file:
            return gr.update(value=file, visible=True), status
        return gr.update(visible=False), status
    
    export_btn.click(handle_export, outputs=[export_file, export_status])

if __name__ == "__main__":
    port = 7860
    
    print("\n" + "="*60)
    print("üé¨ VIDEO EMOTION ANNOTATION TOOL")
    print("="*60)
    print(f"Local URL: http://localhost:{port}")
    print("="*60 + "\n")
    
    demo.queue(max_size=20, default_concurrency_limit=3, api_open=False)
    
    demo.launch(
        share=False,
        server_port=port,
        inbrowser=True,
        show_error=True,
        quiet=False
    )